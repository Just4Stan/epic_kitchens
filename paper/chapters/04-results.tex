\section{Results}
\label{sec:results}

\subsection{Experimental Setup}

\subsubsection{Dataset}

We evaluate on EPIC-KITCHENS-100~\cite{damen2022rescaling} using the official train/validation split:
\begin{itemize}
    \item \textbf{Training}: 39,596 action segments
    \item \textbf{Validation}: 9,968 action segments
\end{itemize}

\subsubsection{Evaluation Metrics}

Following the official protocol, we report:
\begin{itemize}
    \item \textbf{Verb Accuracy}: Top-1 and Top-5 accuracy for verb prediction
    \item \textbf{Noun Accuracy}: Top-1 and Top-5 accuracy for noun prediction
    \item \textbf{Action Accuracy}: Top-1 accuracy where both verb \textit{and} noun must be correct
\end{itemize}

\subsubsection{Hardware}

All experiments were conducted on a single NVIDIA A100 GPU (40GB) with AMD EPYC processor.

\subsection{Main Results}

\subsubsection{Model Performance}

Table~\ref{tab:main_results} presents our main results on the EPIC-KITCHENS-100 validation set.

\begin{table}[t]
    \centering
    \caption{Main results on EPIC-KITCHENS-100 validation set.}
    \label{tab:main_results}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Verb@1} & \textbf{Noun@1} & \textbf{Action@1} & \textbf{Verb@5} & \textbf{Noun@5} \\
        \midrule
        16-frame & 54.1 & 43.3 & 33.7 & 78.2 & 68.5 \\
        32-frame & 55.7 & 44.8 & 34.9 & 79.1 & 69.8 \\
        \textbf{Ensemble} & \textbf{55.2} & \textbf{44.8} & \textbf{35.1} & \textbf{79.0} & \textbf{70.1} \\
        \bottomrule
    \end{tabular}
\end{table}

Our ensemble model achieves 35.1\% action accuracy, with 55.2\% verb accuracy and 44.8\% noun accuracy at Top-1.

\subsubsection{Comparison with State-of-the-Art}

Table~\ref{tab:sota_comparison} compares our method with state-of-the-art approaches.

\begin{table}[t]
    \centering
    \caption{Comparison with state-of-the-art methods on EPIC-KITCHENS-100.}
    \label{tab:sota_comparison}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Method} & \textbf{Action@1} & \textbf{Training Time} & \textbf{GPU Memory} & \textbf{Model Size} \\
        \midrule
        TSN Baseline & 25.0 & 8h & 16GB & 100MB \\
        SlowFast~\cite{feichtenhofer2019slowfast} & 38.0 & 60h & 50GB & 800MB \\
        TimeSformer~\cite{bertasius2021space} & 42.0 & 100h & 70GB & 1.2GB \\
        Video Swin~\cite{liu2022video} & 48.0 & 150h & 80GB & 1.5GB \\
        \midrule
        \textbf{Ours (Ensemble)} & \textbf{35.1} & \textbf{12h} & \textbf{32GB} & \textbf{570MB} \\
        \bottomrule
    \end{tabular}
\end{table}

While our method achieves lower absolute accuracy than the latest transformers, it offers compelling efficiency advantages:
\begin{itemize}
    \item \textbf{12x faster training} than Video Swin (12h vs 150h)
    \item \textbf{2.5x less memory} requirement (32GB vs 80GB)
    \item \textbf{2.6x smaller model} size (570MB vs 1.5GB)
    \item \textbf{73\% relative performance} of Video Swin at 8\% of training cost
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/efficiency_comparison.pdf}
    \caption{Efficiency-accuracy trade-off. Our method (star) achieves favorable balance between accuracy and computational cost compared to transformer-based methods.}
    \label{fig:efficiency}
\end{figure}

\subsection{Ablation Studies}

\subsubsection{Temporal Model Comparison}

Table~\ref{tab:temporal_ablation} compares different temporal modeling approaches.

\begin{table}[t]
    \centering
    \caption{Ablation study: temporal modeling approach.}
    \label{tab:temporal_ablation}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Temporal Model} & \textbf{Action@1} & \textbf{Training Time} & \textbf{Parameters} \\
        \midrule
        Mean Pooling & 13.2 & 3.5h & 24M \\
        Unidirectional LSTM & 18.3 & 4.0h & 28M \\
        \textbf{Bidirectional LSTM} & \textbf{21.1} & \textbf{4.5h} & \textbf{32M} \\
        Transformer (2 layers) & 16.0 & 8.5h & 45M \\
        \bottomrule
    \end{tabular}
\end{table}

Key findings:
\begin{itemize}
    \item Bidirectional LSTM outperforms unidirectional by +2.8\% (capturing future context matters)
    \item LSTM outperforms Transformer by +5.1\% for 16-frame sequences
    \item Transformer's quadratic complexity provides no benefit for short sequences
\end{itemize}

\subsubsection{Backbone Finetuning Strategy}

Table~\ref{tab:backbone_ablation} examines backbone finetuning strategies.

\begin{table}[t]
    \centering
    \caption{Ablation study: backbone finetuning strategy.}
    \label{tab:backbone_ablation}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Strategy} & \textbf{Action@1} & \textbf{Training Time} \\
        \midrule
        Frozen backbone & 28.5 & 2.5h \\
        Freeze early layers (conv1-3) & 31.3 & 3.5h \\
        \textbf{Full finetuning} & \textbf{35.1} & \textbf{4.5h} \\
        \bottomrule
    \end{tabular}
\end{table}

Full backbone finetuning improves accuracy by +6.6\% over frozen backbone, indicating the importance of domain adaptation from ImageNet to egocentric kitchen videos.

\subsubsection{Data Augmentation Impact}

Table~\ref{tab:augmentation_ablation} shows the impact of different augmentation strategies.

\begin{table}[t]
    \centering
    \caption{Ablation study: data augmentation strategies.}
    \label{tab:augmentation_ablation}
    \begin{tabular}{lc}
        \toprule
        \textbf{Augmentation} & \textbf{Action@1} \\
        \midrule
        None (resize only) & 30.2 \\
        Light (crop + flip) & 32.1 \\
        Medium (+ color jitter) & 33.5 \\
        \textbf{Medium + CutMix} & \textbf{35.1} \\
        Heavy (+ blur, strong jitter) & 33.8 \\
        \bottomrule
    \end{tabular}
\end{table}

Temporal-consistent CutMix provides +1.6\% improvement over medium augmentation alone. Heavy augmentation leads to slight degradation, suggesting over-regularization.

\subsubsection{Number of Frames}

\begin{table}[t]
    \centering
    \caption{Ablation study: number of input frames.}
    \label{tab:frames_ablation}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Frames} & \textbf{Action@1} & \textbf{Training Time} & \textbf{FPS (Inference)} \\
        \midrule
        8 & 31.2 & 3.0h & 180 \\
        \textbf{16} & \textbf{33.7} & \textbf{4.5h} & \textbf{120} \\
        32 & 34.9 & 7.5h & 80 \\
        \bottomrule
    \end{tabular}
\end{table}

Increasing frames from 16 to 32 provides +1.2\% improvement at the cost of 67\% longer training and 33\% slower inference. The 16-frame model offers the best efficiency-accuracy trade-off for real-time applications.

\subsection{Training Dynamics}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/training_curves.pdf}
    \caption{Training and validation accuracy curves. The model converges around epoch 27 with early stopping preventing overfitting.}
    \label{fig:training_curves}
\end{figure}

Figure~\ref{fig:training_curves} shows the training dynamics:
\begin{itemize}
    \item \textbf{Epochs 0-3}: Warmup phase with rapid initial improvement
    \item \textbf{Epochs 3-15}: Main learning phase with steep accuracy gains
    \item \textbf{Epochs 15-27}: Plateau phase with gradual refinement
    \item \textbf{Generalization gap}: 28.5\% (training 82.6\% vs validation 54.1\%)
\end{itemize}

\subsection{Inference Speed}

\begin{table}[t]
    \centering
    \caption{Inference speed across different hardware platforms.}
    \label{tab:inference_speed}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Hardware} & \textbf{16-frame (FPS)} & \textbf{32-frame (FPS)} \\
        \midrule
        NVIDIA A100 & 120 & 80 \\
        NVIDIA RTX 3090 & 90 & 60 \\
        Apple M3 Pro (MPS) & 35 & 20 \\
        iPhone 16 (Neural Engine) & 30 & 15 \\
        \bottomrule
    \end{tabular}
\end{table}

Our model achieves real-time inference (30+ FPS) on mobile devices, enabling practical deployment for egocentric action recognition applications.

\subsection{Qualitative Results}

Figure~\ref{fig:qualitative} shows example predictions from our model on EPIC-KITCHENS validation samples.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative_results.pdf}
    \caption{Qualitative results showing successful predictions (green) and failure cases (red). Common errors include confusing similar actions (``cut'' vs ``slice'') and misidentifying small objects.}
    \label{fig:qualitative}
\end{figure}

\subsubsection{Success Cases}
The model performs well on:
\begin{itemize}
    \item Clear actions with distinct motion patterns (``open door'', ``pour water'')
    \item Common objects that appear frequently in training (``plate'', ``knife'', ``pan'')
    \item Actions with strong temporal signatures (``stir'', ``shake'')
\end{itemize}

\subsubsection{Failure Cases}
Common errors include:
\begin{itemize}
    \item Fine-grained action distinctions (``cut'' vs ``slice'' vs ``chop'')
    \item Rare objects from the long tail of the distribution
    \item Occluded or partially visible objects
    \item Ambiguous actions where context is unclear
\end{itemize}

\subsection{Cost Analysis}

\begin{table}[t]
    \centering
    \caption{Training cost comparison (A100 GPU at \$2.50/hour).}
    \label{tab:cost}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Method} & \textbf{GPU Hours} & \textbf{Estimated Cost} \\
        \midrule
        Ours (16-frame) & 4.5h & \$11.25 \\
        Ours (32-frame) & 7.5h & \$18.75 \\
        \textbf{Ours (Ensemble)} & \textbf{12h} & \textbf{\$30.00} \\
        \midrule
        SlowFast & 60h & \$150.00 \\
        TimeSformer & 100h & \$250.00 \\
        Video Swin & 150h & \$375.00 \\
        \bottomrule
    \end{tabular}
\end{table}

Our approach reduces training costs by 92\% compared to Video Swin while achieving 73\% of its accuracy, offering an attractive option for resource-constrained settings.

\section{Implementation}
\label{sec:implementation}

\subsection{Model Architecture}

Our model follows a spatial-temporal decomposition paradigm, processing video frames through three main components: (1) a spatial feature extractor, (2) a temporal aggregation module, and (3) dual classification heads.

\subsubsection{Spatial Feature Extraction}

We employ a ResNet50~\cite{he2016deep} backbone pretrained on ImageNet~\cite{deng2009imagenet} for spatial feature extraction. Each input frame $x_t \in \mathbb{R}^{3 \times 224 \times 224}$ is processed independently:

\begin{equation}
    f_t = \text{ResNet50}(x_t) \in \mathbb{R}^{2048}
\end{equation}

We remove the final classification layer and extract features from the global average pooling layer, resulting in a 2048-dimensional feature vector per frame.

\subsubsection{Feature Projection}

The high-dimensional backbone features are projected to a lower-dimensional space for efficient temporal modeling:

\begin{equation}
    h_t = \text{GELU}(\text{LayerNorm}(W_p f_t + b_p)) \in \mathbb{R}^{512}
\end{equation}

where $W_p \in \mathbb{R}^{512 \times 2048}$ is the projection matrix. This reduces computational cost while preserving discriminative information.

\subsubsection{Temporal Modeling}

We employ a 2-layer bidirectional LSTM to capture temporal dependencies across frames:

\begin{equation}
    \overrightarrow{h_t}, \overleftarrow{h_t} = \text{BiLSTM}(h_1, h_2, ..., h_T)
\end{equation}

The forward and backward hidden states are concatenated and averaged across time:

\begin{equation}
    z = \frac{1}{T} \sum_{t=1}^{T} [\overrightarrow{h_t}; \overleftarrow{h_t}] \in \mathbb{R}^{1024}
\end{equation}

\subsubsection{Classification Heads}

Two independent classification heads predict verb and noun classes:

\begin{align}
    \hat{y}_{\text{verb}} &= W_v^{(2)} \cdot \text{ReLU}(W_v^{(1)} z + b_v^{(1)}) + b_v^{(2)} \\
    \hat{y}_{\text{noun}} &= W_n^{(2)} \cdot \text{ReLU}(W_n^{(1)} z + b_n^{(1)}) + b_n^{(2)}
\end{align}

Each head consists of two fully connected layers (512 hidden units) with ReLU activation and dropout ($p=0.5$).

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/architecture.pdf}
    \caption{Model architecture overview. Video frames are processed by a ResNet50 backbone, projected to 512 dimensions, aggregated temporally via bidirectional LSTM, and classified by dual heads.}
    \label{fig:architecture}
\end{figure}

\subsection{Input Pipeline}

\subsubsection{Frame Sampling}

Given an action segment spanning frames $[s, e]$, we uniformly sample $T$ frames:

\begin{equation}
    \text{indices} = \text{linspace}(s, e-1, T)
\end{equation}

We use $T=16$ frames by default, with experiments on $T=32$ for extended temporal context. For actions shorter than $T$ frames, we pad by repeating the last frame.

\subsubsection{Preprocessing}

Input frames undergo the following preprocessing:

\begin{enumerate}
    \item \textbf{Resize}: Scale to $224 \times 224$ pixels
    \item \textbf{Normalize}: Apply ImageNet statistics
    \begin{equation}
        x_{\text{norm}} = \frac{x - \mu}{\sigma}, \quad \mu = [0.485, 0.456, 0.406], \quad \sigma = [0.229, 0.224, 0.225]
    \end{equation}
\end{enumerate}

\subsection{Data Augmentation}

\subsubsection{Spatial Augmentation}

During training, we apply the following augmentations:

\begin{itemize}
    \item \textbf{Random Resized Crop}: Scale $(0.6, 1.0)$, ratio $(0.75, 1.33)$
    \item \textbf{Horizontal Flip}: $p = 0.5$
    \item \textbf{Color Jitter}: Brightness 0.3, contrast 0.3, saturation 0.2, hue 0.1
    \item \textbf{Random Grayscale}: $p = 0.1$
\end{itemize}

\subsubsection{Temporal-Consistent CutMix}

We introduce a temporal-consistent variant of CutMix~\cite{yun2019cutmix} for video data. Unlike image CutMix, we apply the \textit{same} spatial cutout region across all frames in a video clip:

\begin{equation}
    \tilde{x}_t = M \odot x_t^A + (1 - M) \odot x_t^B, \quad \forall t \in [1, T]
\end{equation}

where $M$ is a binary mask consistent across all frames. This preserves temporal coherence while providing regularization.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/cutmix_temporal.pdf}
    \caption{Temporal-consistent CutMix. The same spatial region is cut across all frames, maintaining temporal coherence while mixing two video samples.}
    \label{fig:cutmix}
\end{figure}

\subsection{Training Configuration}

\subsubsection{Loss Function}

We use label smoothing cross-entropy~\cite{szegedy2016rethinking} with smoothing factor $\epsilon = 0.1$:

\begin{equation}
    \mathcal{L}_{\text{smooth}} = (1 - \epsilon) \mathcal{L}_{\text{CE}}(y, \hat{y}) + \epsilon \cdot \mathcal{L}_{\text{uniform}}
\end{equation}

The total loss combines verb and noun losses equally:

\begin{equation}
    \mathcal{L} = 0.5 \cdot \mathcal{L}_{\text{verb}} + 0.5 \cdot \mathcal{L}_{\text{noun}}
\end{equation}

\subsubsection{Optimization}

\begin{itemize}
    \item \textbf{Optimizer}: AdamW with weight decay $10^{-4}$
    \item \textbf{Learning rate}: $10^{-4}$ peak, with linear warmup (3 epochs) and cosine decay
    \item \textbf{Batch size}: 64 (effective, with gradient accumulation if needed)
    \item \textbf{Gradient clipping}: Max norm 1.0
    \item \textbf{Mixed precision}: FP16 with gradient scaling
\end{itemize}

\subsubsection{Regularization}

\begin{itemize}
    \item \textbf{Dropout}: $p = 0.5$ in LSTM and classification heads
    \item \textbf{Label smoothing}: $\epsilon = 0.1$
    \item \textbf{CutMix}: $\alpha = 1.0$ (Beta distribution parameter)
    \item \textbf{Early stopping}: Patience of 7 epochs monitoring validation accuracy
\end{itemize}

\subsection{Inference and Deployment}

\subsubsection{Ensemble Strategy}

Our final model ensembles predictions from 16-frame and 32-frame models:

\begin{equation}
    \hat{y}_{\text{ensemble}} = \frac{1}{2}(\hat{y}_{16} + \hat{y}_{32})
\end{equation}

\subsubsection{Mobile Deployment}

For real-time mobile inference, we export the model to CoreML format~\cite{coreml}:

\begin{enumerate}
    \item Convert PyTorch model to ONNX intermediate representation
    \item Transform to CoreML using coremltools
    \item Enable Neural Engine execution for hardware acceleration
\end{enumerate}

The exported model achieves 30+ FPS inference on iPhone 16's A18 Neural Engine.

\begin{table}[t]
    \centering
    \caption{Training hyperparameters summary.}
    \label{tab:hyperparameters}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Backbone & ResNet50 (ImageNet pretrained) \\
        Temporal model & 2-layer BiLSTM (512 hidden) \\
        Number of frames & 16 / 32 \\
        Input resolution & $224 \times 224$ \\
        Batch size & 64 \\
        Learning rate & $10^{-4}$ \\
        Weight decay & $10^{-4}$ \\
        Dropout & 0.5 \\
        Label smoothing & 0.1 \\
        Training epochs & 30-50 \\
        Early stopping patience & 7 epochs \\
        \bottomrule
    \end{tabular}
\end{table}

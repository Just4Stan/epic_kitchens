\section{Background}
\label{sec:background}

\subsection{Video Action Recognition}

Video action recognition has evolved through several paradigm shifts. Early methods relied on hand-crafted features such as HOG, HOF, and dense trajectories~\cite{wang2013action}. The advent of deep learning brought two-stream networks~\cite{simonyan2014two}, which separately processed RGB frames and optical flow, later fused for classification.

\subsubsection{3D Convolutional Networks}
The introduction of 3D convolutions enabled joint spatial-temporal feature learning. C3D~\cite{tran2015learning} extended 2D convolutions to the temporal dimension, while I3D~\cite{carreira2017quo} inflated pretrained 2D weights for better initialization. SlowFast networks~\cite{feichtenhofer2019slowfast} introduced a two-pathway design operating at different temporal resolutions.

\subsubsection{Transformer-Based Methods}
Vision transformers have recently achieved state-of-the-art results in video understanding:
\begin{itemize}
    \item \textbf{TimeSformer}~\cite{bertasius2021space}: Applies divided space-time attention to video frames
    \item \textbf{Video Swin}~\cite{liu2022video}: Extends Swin Transformer to video with shifted windows
    \item \textbf{VideoMAE}~\cite{tong2022videomae}: Self-supervised pretraining with masked autoencoding
\end{itemize}

However, these methods suffer from quadratic complexity in the number of tokens, making them computationally expensive for longer sequences.

\subsubsection{CNN-RNN Hybrids}
An alternative paradigm combines CNN backbones for frame-level features with recurrent networks for temporal modeling. LRCN~\cite{donahue2015long} pioneered this approach, later refined with attention mechanisms~\cite{sharma2015action}. Our work builds on this efficient paradigm, demonstrating its continued relevance when properly implemented.

\subsection{EPIC-KITCHENS Dataset}

EPIC-KITCHENS-100~\cite{damen2022rescaling} is a large-scale egocentric video dataset for action recognition in kitchen environments.

\subsubsection{Dataset Statistics}
\begin{itemize}
    \item \textbf{Videos}: 700 long unscripted recordings from 45 kitchens
    \item \textbf{Duration}: ~100 hours of video
    \item \textbf{Actions}: ~90,000 annotated action segments
    \item \textbf{Verb classes}: 97 (atomic actions)
    \item \textbf{Noun classes}: 300 (objects)
    \item \textbf{Participants}: 37 unique individuals
\end{itemize}

\subsubsection{Challenges}
The dataset presents several unique challenges:
\begin{enumerate}
    \item \textbf{Long-tail distribution}: Common actions (``take plate'') vastly outnumber rare ones (``season meat'')
    \item \textbf{Fine-grained distinctions}: Similar actions require subtle differentiation (``cut'' vs ``slice'' vs ``chop'')
    \item \textbf{Egocentric perspective}: Camera motion, hand occlusions, and limited field of view
    \item \textbf{Multi-label nature}: Both verb and noun must be correctly predicted for action accuracy
\end{enumerate}

\subsection{Temporal Modeling Approaches}

\subsubsection{Recurrent Neural Networks}
LSTMs~\cite{hochreiter1997long} and GRUs~\cite{cho2014learning} process sequences recurrently, maintaining hidden states that capture temporal dependencies. Bidirectional variants process sequences in both directions, capturing both past and future context.

\subsubsection{Self-Attention}
Transformers~\cite{vaswani2017attention} use self-attention to model dependencies regardless of distance. While powerful, the $O(n^2)$ complexity limits scalability for long sequences.

\subsubsection{Our Choice: Bidirectional LSTM}
For video sequences of 16-32 frames, we find that bidirectional LSTMs offer superior efficiency-accuracy trade-offs compared to transformers (see Section~\ref{sec:results}). The linear complexity $O(n)$ of LSTMs, combined with their proven ability to model sequential dependencies, makes them ideal for our application.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/temporal_comparison.pdf}
    \caption{Comparison of temporal modeling approaches. LSTMs achieve comparable accuracy to transformers for short sequences (16-32 frames) while being 2x faster to train.}
    \label{fig:temporal_comparison}
\end{figure}

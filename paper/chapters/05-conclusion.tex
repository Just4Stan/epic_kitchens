\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary}

We presented an efficient spatial-temporal approach for egocentric action recognition on EPIC-KITCHENS-100. Our method combines a ResNet50 backbone with a bidirectional LSTM for temporal modeling, achieving 35.1\% action accuracy while requiring only 12 hours of training on a single A100 GPU.

\subsection{Key Findings}

Our experiments revealed several important insights:

\begin{enumerate}
    \item \textbf{LSTMs outperform Transformers for short sequences}: For 16-32 frame video clips, bidirectional LSTMs achieve higher accuracy than transformer-based temporal models while being 2x faster to train. The quadratic complexity of self-attention provides no benefit for short sequences.

    \item \textbf{Full backbone finetuning is critical}: Adapting the ImageNet-pretrained backbone to the egocentric kitchen domain improves accuracy by +6.6\% compared to frozen features, indicating significant domain shift.

    \item \textbf{Temporal-consistent augmentation matters}: Our CutMix variant that applies the same spatial mask across all frames provides +1.6\% improvement, highlighting the importance of maintaining temporal coherence during augmentation.

    \item \textbf{Efficiency-accuracy trade-offs are achievable}: Our method achieves 73\% of Video Swin's accuracy at 8\% of the training cost, demonstrating that simpler architectures remain competitive when properly implemented.
\end{enumerate}

\subsection{Practical Impact}

Our approach enables practical deployment of egocentric action recognition:

\begin{itemize}
    \item \textbf{Real-time inference}: 30+ FPS on consumer hardware and mobile devices
    \item \textbf{Low training cost}: \$30 vs \$375+ for transformer-based alternatives
    \item \textbf{Accessible compute}: Single-GPU training within 12 hours
    \item \textbf{Mobile deployment}: Successful CoreML export for iPhone/iPad applications
\end{itemize}

\subsection{Limitations}

Our approach has several limitations:

\begin{enumerate}
    \item \textbf{Lower absolute accuracy}: We achieve 35.1\% vs 48\% for Video Swin, representing a meaningful gap for applications requiring maximum accuracy.

    \item \textbf{Long-tail performance}: Like most methods, we struggle with rare verb-noun combinations that have few training examples.

    \item \textbf{Fine-grained distinctions}: Subtle action differences (e.g., ``cut'' vs ``slice'') remain challenging without explicit modeling of motion trajectories.

    \item \textbf{Single modality}: We only use RGB frames; incorporating optical flow or audio could improve performance.
\end{enumerate}

\subsection{Future Work}

Several directions could extend this work:

\begin{enumerate}
    \item \textbf{Multi-modal fusion}: Incorporating optical flow, audio, and object detection features

    \item \textbf{Class-balanced learning}: Addressing the long-tail distribution through balanced sampling or loss re-weighting

    \item \textbf{Self-supervised pretraining}: Leveraging large-scale unlabeled egocentric video for pretraining

    \item \textbf{Action anticipation}: Extending to predict future actions before they occur

    \item \textbf{Efficient transformers}: Exploring linear-complexity attention mechanisms that could combine transformer expressiveness with LSTM efficiency
\end{enumerate}

\subsection{Conclusion}

In the era of increasingly large models, our work demonstrates that well-designed CNN-RNN architectures remain competitive for video understanding tasks. By carefully balancing accuracy and efficiency, we enable practical egocentric action recognition applications that were previously limited by computational constraints. We hope our work encourages further research into efficient video understanding methods that can be deployed in real-world settings.

\section*{Acknowledgments}

We thank the EPIC-KITCHENS team for providing the dataset and evaluation infrastructure. Experiments were conducted on [computing infrastructure].

\section*{Code Availability}

Code and pretrained models are available at: \url{https://github.com/[repository]}
